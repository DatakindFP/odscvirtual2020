---
title: ML in R
date: April 14, 2020
output:
    html_document:
        toc: true
---

# Data

```{r}
uk <- readr::read_csv(here::here('data', 'Comp_UK.csv'))
uk
```

```{r}
library(magrittr)
comps <- dir(here::here('data'), pattern='^Comp_', full.names=TRUE) %>% 
    purrr::map_df(readr::read_csv)
comps
```

# EDA

```{r}
library(ggplot2)
ggplot(comps, aes(x=SalaryCY)) + geom_histogram()
ggplot(comps, aes(x=SalaryCY, fill=Title)) + geom_histogram()
```

# Split the Data

```{r}
library(rsample)
comp_split <- initial_split(comps, prop=0.8, strata='SalaryCY')
comp_split
train <- training(comp_split)
test <- testing(comp_split)
```

# Terminology

- Outputs: y, response, target, label, ~depedent variable~
- Inputs: x, covartiate, feature, predictor, ~independent variable~
- Intercept/Bias
- Coefficients/Weights
- Inference/<NA>
- Prediction/Inference

# Lasso

$$
y \sim a + b_1x_1 + b_2x_2 + \cdots + b_px_p + \epsilon \\
\epsilon \sim N(0, \sigma)
$$

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum |\beta_j| \right ]
$$

```{r}
library(glmnet)
?glmnet
```

```{r}
names(train)
mod1 <- lm(
    SalaryCY ~ . - ID - SalaryPY - BonusCY - BonusPY, data=train
)
mod1
library(coefplot)
coefplot(mod1, sort='magnitude')
```

```{r}
# mod2 <- glmnet(formula, data)
# mod2 <- glmnet(x_matrix, y_matrix)
```

# Recipes

Standardization, normalization, centering and scaling

$$
\tilde{x} = \frac{x - \bar{x}}{sd(x)}
$$

clothing: shirt, hat, pants
shirt*7

typically, q levels in a categorical variable needs q - 1 dummy variables

but in ML, we don't care, so we use q dummy variables

```{r}
library(recipes)

rec1 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_other(all_nominal(), threshold=0.01) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)

rec1

prep1 <- prep(rec1, training=train)
prep1
```

```{r}
bake(prep1, new_data=train)
bake(prep1, new_data=test)
```

