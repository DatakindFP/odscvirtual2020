---
title: ML in R
date: April 14, 2020
output:
    html_document:
        toc: true
---

# Data

```{r}
uk <- readr::read_csv(here::here('data', 'Comp_UK.csv'))
uk
```

```{r}
library(magrittr)
comps <- dir(here::here('data'), pattern='^Comp_', full.names=TRUE) %>% 
    purrr::map_df(readr::read_csv)
comps
```

# EDA

```{r}
library(ggplot2)
ggplot(comps, aes(x=SalaryCY)) + geom_histogram()
ggplot(comps, aes(x=SalaryCY, fill=Title)) + geom_histogram()
```

# Split the Data

```{r}
library(rsample)
comp_split <- initial_split(comps, prop=0.8, strata='SalaryCY')
comp_split
train <- training(comp_split)
test <- testing(comp_split)
```

# Terminology

- Outputs: y, response, target, label, ~depedent variable~
- Inputs: x, covartiate, feature, predictor, ~independent variable~
- Intercept/Bias
- Coefficients/Weights
- Inference/<NA>
- Prediction/Inference

# Lasso

$$
y \sim a + b_1x_1 + b_2x_2 + \cdots + b_px_p + \epsilon \\
\epsilon \sim N(0, \sigma)
$$

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum |\beta_j| \right ]
$$

```{r}
library(glmnet)
?glmnet
```

```{r}
names(train)
mod1 <- lm(
    SalaryCY ~ . - ID - SalaryPY - BonusCY - BonusPY, data=train
)
mod1
library(coefplot)
coefplot(mod1, sort='magnitude')
```

```{r}
# mod2 <- glmnet(formula, data)
# mod2 <- glmnet(x_matrix, y_matrix)
```

# Recipes

Standardization, normalization, centering and scaling

$$
\tilde{x} = \frac{x - \bar{x}}{sd(x)}
$$

clothing: shirt, hat, pants
shirt*7

typically, q levels in a categorical variable needs q - 1 dummy variables

but in ML, we don't care, so we use q dummy variables

colors: blue, blue, blue, red, green, blue, green, green, red, red, blue, violet, blue, red, green, yellow, blue, pink, violet, blue, red, green

colors: blue, blue, blue, red, green, blue, green, green, red, red, blue, other, blue, red, green, other, blue, other, other, blue, red, green

```{r}
library(recipes)

rec1 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_nzv(all_predictors()) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_other(all_nominal(), threshold=0.01) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)

rec1

prep1 <- prep(rec1, training=train)
prep1
```

```{r}
bake(prep1, new_data=train)
bake(prep1, new_data=test)
```

```{r}
train_x <- bake(prep1, 
                new_data=train, all_predictors(), composition='matrix')
head(train_x)
train_y <- bake(prep1, 
                new_data=train, all_outcomes(), composition='matrix')
head(train_y)
```

# Back to Lasso

```{r}
mod2 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=1)
plot(mod2, xvar='lambda')
coefpath(mod2)
```

```{r}
mod2$lambda
coefplot(mod2, sort='magnitude', lambda=1872)
coefplot(mod2, sort='magnitude', lambda=657544)
coefplot(mod2, sort='magnitude', lambda=9994)
coefplot(mod2, sort='magnitude', lambda=3591)
```

```{r}
mod3 <- cv.glmnet(x=train_x, y=train_y, family='gaussian', alpha=1,
                  nfolds=5)
coefplot(mod3, sort='magnitude', lambda='lambda.min')
```

# Ridge

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum \beta_j^2 \right ]
$$
```{r}
mod4 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=0)
coefpath(mod4)
```

# Elastic Net

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \left( \alpha \sum |\beta_j| + (1-\alpha)\frac{1}{2} \sum \beta_j^2 \right) \right ]
$$


```{r}
mod5 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=0.7)
coefpath(mod5)
```



