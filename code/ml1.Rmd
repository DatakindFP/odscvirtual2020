---
title: ML in R
date: April 14, 2020
output:
    html_document:
        toc: true
---

# Data

```{r}
uk <- readr::read_csv(here::here('data', 'Comp_UK.csv'))
uk
```

```{r}
library(magrittr)
comps <- dir(here::here('data'), pattern='^Comp_', full.names=TRUE) %>% 
    purrr::map_df(readr::read_csv)
comps
```

# EDA

```{r}
library(ggplot2)
ggplot(comps, aes(x=SalaryCY)) + geom_histogram()
ggplot(comps, aes(x=SalaryCY, fill=Title)) + geom_histogram()
```

# Split the Data

```{r}
library(rsample)
comp_split <- initial_split(comps, prop=0.8, strata='SalaryCY')
comp_split
train <- training(comp_split)
test <- testing(comp_split)
```

# Terminology

- Outputs: y, response, target, label, ~depedent variable~
- Inputs: x, covartiate, feature, predictor, ~independent variable~
- Intercept/Bias
- Coefficients/Weights
- Inference/<NA>
- Prediction/Inference

# Lasso

$$
y \sim a + b_1x_1 + b_2x_2 + \cdots + b_px_p + \epsilon \\
\epsilon \sim N(0, \sigma)
$$

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum |\beta_j| \right ]
$$

```{r}
library(glmnet)
?glmnet
```

```{r}
names(train)
mod1 <- lm(
    SalaryCY ~ . - ID - SalaryPY - BonusCY - BonusPY, data=train
)
mod1
library(coefplot)
coefplot(mod1, sort='magnitude')
```

```{r}
# mod2 <- glmnet(formula, data)
# mod2 <- glmnet(x_matrix, y_matrix)
```

# Recipes

Standardization, normalization, centering and scaling

$$
\tilde{x} = \frac{x - \bar{x}}{sd(x)}
$$

clothing: shirt, hat, pants
shirt*7

typically, q levels in a categorical variable needs q - 1 dummy variables

but in ML, we don't care, so we use q dummy variables

colors: blue, blue, blue, red, green, blue, green, green, red, red, blue, violet, blue, red, green, yellow, blue, pink, violet, blue, red, green

colors: blue, blue, blue, red, green, blue, green, green, red, red, blue, other, blue, red, green, other, blue, other, other, blue, red, green

```{r}
library(recipes)

rec1 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_nzv(all_predictors()) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_other(all_nominal(), threshold=0.01) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)

rec1

prep1 <- prep(rec1, training=train)
prep1
```

```{r}
bake(prep1, new_data=train)
bake(prep1, new_data=test)
```

```{r}
train_x <- bake(prep1, 
                new_data=train, all_predictors(), composition='matrix')
head(train_x)
train_y <- bake(prep1, 
                new_data=train, all_outcomes(), composition='matrix')
head(train_y)
```

# Back to Lasso

```{r}
mod2 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=1)
plot(mod2, xvar='lambda')
coefpath(mod2)
```

```{r}
mod2$lambda
coefplot(mod2, sort='magnitude', lambda=1872)
coefplot(mod2, sort='magnitude', lambda=657544)
coefplot(mod2, sort='magnitude', lambda=9994)
coefplot(mod2, sort='magnitude', lambda=3591)
```

```{r}
mod3 <- cv.glmnet(x=train_x, y=train_y, family='gaussian', alpha=1,
                  nfolds=5)
coefplot(mod3, sort='magnitude', lambda='lambda.min')
```

# Ridge

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \sum \beta_j^2 \right ]
$$
```{r}
mod4 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=0)
coefpath(mod4)
```

# Elastic Net

$$
\hat{\beta} = argmin \left [ \sum (y_i - X_i\beta)^2 + \lambda \left( \alpha \sum |\beta_j| + (1-\alpha)\frac{1}{2} \sum \beta_j^2 \right) \right ]
$$


```{r}
mod5 <- glmnet(x=train_x, y=train_y, family='gaussian', alpha=0.7)
coefpath(mod5)
```

# Penalized Regression Summary

- Ordinary regression with special features and interpretable
- Shrinks coefficients toward 0 for more realistc effects
- Performs automated variable selection
- two hyperparameters
    - lambda: amount of penalty
    - alpha: type of penalty: lasso vs ridge

# Trees

- regression
- classification

## Packages

- `{rpart}`
- `{party}`
- `{C5.0}`
- `{xgboost}`

## Benefits

- simple trees are easy to understand
- good predictions

## Flaws

- Complex trees are hard to understand
- prone to overfitting

## Improvements

- Random Forests
- Boosting

gradient boosted machine

# xgboost

```{r}
library(xgboost)
trainxg <- xgb.DMatrix(data=train_x, label=train_y)
trainxg
```

```{r}
mod6 <- xgb.train(
    data=trainxg,
    nrounds=1, 
    objective='reg:squarederror',
    booster='gbtree' # gblinear
)

# DiagrammeR
?xgb.train
xgb.plot.multi.trees(mod6)
```

```{r}
mod7 <- xgb.train(
    data=trainxg,
    nrounds=100, 
    objective='reg:squarederror',
    booster='gbtree' # gblinear
)

xgb.plot.multi.trees(mod7)
```

```{r}
mod8 <- xgb.train(
    data=trainxg,
    nrounds=100, 
    objective='reg:squarederror',
    booster='gbtree',
    watchlist=list(train=trainxg),
    print_every_n=1
)

mod9 <- xgb.train(
    data=trainxg,
    nrounds=500, 
    objective='reg:squarederror',
    booster='gbtree',
    watchlist=list(train=trainxg),
    print_every_n=1
)
```

```{r}
val_x <- bake(prep1, new_data=test, all_predictors(), composition='matrix')
val_y <- bake(prep1, new_data=test, all_outcomes(), composition='matrix')
valxg <- xgb.DMatrix(data=val_x, label=val_y)
```

```{r}
mod10 <- xgb.train(
    data=trainxg,
    nrounds=500, 
    objective='reg:squarederror',
    booster='gbtree',
    watchlist=list(train=trainxg, validate=valxg),
    print_every_n=1
)

library(dygraphs)
dygraph(mod10$evaluation_log)
```

```{r}
mod11 <- xgb.train(
    data=trainxg,
    nrounds=500, 
    objective='reg:squarederror',
    booster='gbtree',
    watchlist=list(train=trainxg, validate=valxg),
    print_every_n=1,
    max_depth=3
)

dygraph(mod11$evaluation_log)

mod10$evaluation_log[validate_rmse == min(validate_rmse), ]
mod11$evaluation_log[validate_rmse == min(validate_rmse), ]
```

```{r}
xgb.importance(model=mod11) %>% 
    .[1:10, ] %>% 
    xgb.plot.importance()
```

# Predictions

## glmnet

```{r}
preds5 <- predict(mod5, newx=val_x, s=6534)
head(preds5)
```

```{r}
preds11 <- predict(mod11, newdata=val_x)
head(preds11)

preds11.a <- predict(mod11, newdata=val_x, ntreelimit=76)
head(preds11.a)
```

