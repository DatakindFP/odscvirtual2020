---
title: ML Workflows in R
date: April 14, 2020
author: Jared P. Lander
output:
    html_document:
        toc: true
---

# Workflow

1. Read in the data
1. Spit the data
    - train
    - test
1. Resampling
    - cross-validation
    - bootstrap
1. Feature Engineering
1. Model Specification
1. Define Evaluation Metrics
1. Select Best Model
1. Fit a final model
1. Make Predictions

# Read the Data

```{r}
library(magrittr)
comps <- dir(here::here('data'), pattern='^Comp_', full.names=TRUE) %>% 
    purrr::map_df(readr::read_csv)
comps
```

# Split the Data

```{r}
library(rsample)
comp_split <- initial_split(comps, prop=0.8, strata='SalaryCY')
comp_split
train <- training(comp_split)
test <- testing(comp_split)
```

# Resampling

```{r}
the_cv <- vfold_cv(data=train, v=4, repeats=2, strata='SalaryCY')
the_cv
the_cv$splits[[1]]
the_cv$splits[[1]]
```

```{r}
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[1]] %>% training())
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[2]] %>% training())
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[3]] %>% training())
```

# Feature Engineering

```{r}
library(recipes)
library(tune)

train$Title %>% table()

rec1 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_nzv(all_predictors()) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_naomit(all_outcomes()) %>% 
    step_BoxCox(Floor) %>% 
    step_bs(Years, deg_free=tune()) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_upsample(Title, over_ratio=0.5) %>% 
    step_other(all_nominal()) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)

rec1

rec1 %>% parameters()

# won't work because of tuning parameter
# prep(rec1, training=train)
```

# Model Specification

```{r eval=FALSE}
lm(formula=y ~ x, data=data)
glm(formula=y ~ x, data=data)
glmnet(x=x_matrix, y=y_matrix)
xgb.train(data=xgb.DMatrix(data=x_matrix, label=y_matrix))
```

1. type of model
1. computation engine
1. fitting the model

```{r}
library(parsnip)

linear_reg()
logistic_reg()

decision_tree()
decision_tree(mode='regression')
decision_tree(mode='classification')

rand_forest()

boost_tree(mode='regression')
```

```{r}
linear_reg() %>% set_engine('lm')
linear_reg() %>% set_engine('glmnet')
linear_reg() %>% set_engine('stan')
linear_reg() %>% set_engine('keras')
linear_reg() %>% set_engine('spark')
```

```{r}
lm_spec <- linear_reg() %>% set_engine('lm')

mod1 <- lm_spec %>% fit(SalaryCY ~ Years + Title, data=train)
mod1
mod1 %>% class

net_spec <- linear_reg() %>% set_engine('glmnet')
mod2 <- net_spec %>% fit(SalaryCY ~ Years + Title, data=train)
mod2 %>% class
library(coefplot)
mod2$fit %>% coefpath()
```


```{r}
lm_spec %>% fit(SalaryCY ~ Years + Title, data=train)
net_spec %>% fit(SalaryCY ~ Years + Title, data=train)
```

```{r eval=FALSE}
# won't work because of tuning parameter
net_spec %>% 
    fit(
        SalaryCY ~ ., 
        data=rec1 %>% prep(training=train) %>% bake(newdata=train)
    )
```


```{r}
linear_reg(penalty=6537, mixture=1) %>% set_engine('glmnet')

boost_tree(trees=100)

net_spec <- linear_reg(penalty=tune(), mixture=1) %>% set_engine('glmnet')
net_spec %>% parameters()
```

# Evaluation Metrics

- ~AIC~
- ~BIC~
- ~r-squared~
- root mean squared error (rmse)
- mean absolute error (mae)

$$
rmse = \frac{1}{n}\sqrt{\sum (y_i - \hat{y}_i)^2}
$$

$$
mae = \frac{1}{n} \sum |y_i - \hat{y}_i|
$$

0/1

FALSE/TRUE

Lose/Win

```{r}
library(yardstick)


small_results <- tibble::tibble(true=c(2, 5, 3), predicted=c(3, 4, 3))
small_results

rmse(small_results, truth=true, estimate=predicted)
mae(small_results, truth=true, estimate=predicted)

the_metrics <- metric_set(rmse, mae)
the_metrics
the_metrics(small_results, truth=true, estimate=predicted)
```

```{r}
glm_spec <- linear_reg(penalty=6437) %>% set_engine('glmnet')
mod2 <- glm_spec %>% fit(SalaryCY ~ Title + Reports, data=train)
preds2 <- predict(mod2, new_data=test)
preds2
preds2 %>% class

library(dplyr)
test %>% 
    select(SalaryCY) %>% 
    bind_cols(preds2) %>% 
    the_metrics(truth=SalaryCY, estimate=.pred)
```

# Hyperparameters

```{r}
rec1 %>% parameters()
net_spec %>% parameters()

library(dials)
deg_free()
penalty()

trees()
tree_depth()
```

# Workflows

```{r}
rec2 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_nzv(all_predictors()) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_naomit(all_outcomes()) %>% 
    step_BoxCox(Floor) %>% 
    step_bs(Years, deg_free=5) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_upsample(Title, over_ratio=0.5) %>% 
    step_other(all_nominal()) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)
```


```{r}
library(workflows)

flow_test <- workflow() %>% 
    add_recipe(rec2) %>% 
    add_model(glm_spec)
flow_test

mod3 <- flow_test %>% fit(train)
mod3 %>% class
mod3$fit$fit$fit %>% coefpath()

# preds3 <- predict(mod3, new_data=test)
```

```{r}
flow4 <- workflow() %>% 
    add_recipe(rec1) %>% 
    add_model(net_spec)
flow4

# gives error
# mod4 <- flow4 %>% fit(train)
```

```{r}
flow4 %>% parameters()

params4 <- flow4 %>% 
    parameters() %>% 
    update(
        deg_free=deg_free(range=c(3, 6))
    )

```

# Search Grid

```{r}
grid4 <- grid_random(params4, size=20)
grid4
```

# Tuning

```{r}
tune4 <- tune_grid(
    flow4,
    resamples=the_cv,
    grid=grid4,
    metrics=the_metrics,
    control=control_grid(verbose=TRUE, allow_par=TRUE)
)
```

```{r}
tune4
tune4$.notes[[2]]
tune4$.metrics[[1]]
tune4$.metrics[[2]]

tune4 %>% autoplot()

tune4 %>% collect_metrics() %>% 
    filter(.metric=='mae') %>% 
    arrange(mean)

tune4 %>% autoplot()
tune4 %>% autoplot(metric='mae')

tune4 %>% show_best(metric='mae')

tune4 %>% select_best(metric='mae')
tune4 %>% select_by_one_std_err(metric='mae', penalty, deg_free)

best_params <- tune4 %>% select_by_one_std_err(metric='mae', penalty, deg_free)
best_params
```

# Finalize Model

```{r}
flow_final <- flow4 %>% 
    finalize_workflow(best_params)

flow_final
```

# Evaluate Our Flow

```{r}
mod5 <- flow_final %>% 
    fit(train)
mod5$fit$fit$fit$lambda
mod5
```

```{r}
undebug(workflows:::predict.workflow)
# preds5 <- predict(mod5, new_data=test)

# don't do this
preds5 <- predict(
    mod5$fit$fit$fit,
    newx=rec1 %>% 
        finalize_recipe(best_params) %>% 
        prep(training=train) %>% 
        bake(new_data=test, all_predictors(), composition='matrix'),
    s=10^(best_params$penalty)
)
head(preds5)

test %>% 
    select(SalaryCY) %>% 
    bind_cols(preds5) %>% 
    the_metrics(truth=SalaryCY, estimate=.preds)
```

# Fit on all Data

```{r}
mod_final <- flow_final %>% 
    fit(comps)
```

# More Ways to Tune

```{r}
tune6 <- tune_bayes(
    flow4,
    resamples=the_cv,
    iter=25,
    metrics=the_metrics,
    param_info=params4,
    control=control_bayes(verbose=TRUE, uncertain=5, time_limit=5),
    initial=4
)

tune4 %>% show_best(metric='mae')
tune6 %>% show_best(metric='mae')
```


