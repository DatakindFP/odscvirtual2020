---
title: ML Workflows in R
date: April 14, 2020
author: Jared P. Lander
output:
    html_document:
        toc: true
---

# Workflow

1. Read in the data
1. Spit the data
    - train
    - test
1. Resampling
    - cross-validation
    - bootstrap
1. Feature Engineering
1. Model Specification
1. Define Evaluation Metrics
1. Select Best Model
1. Fit a final model
1. Make Predictions

# Read the Data

```{r}
library(magrittr)
comps <- dir(here::here('data'), pattern='^Comp_', full.names=TRUE) %>% 
    purrr::map_df(readr::read_csv)
comps
```

# Split the Data

```{r}
library(rsample)
comp_split <- initial_split(comps, prop=0.8, strata='SalaryCY')
comp_split
train <- training(comp_split)
test <- testing(comp_split)
```

# Resampling

```{r}
the_cv <- vfold_cv(data=train, v=4, repeats=2, strata='SalaryCY')
the_cv
the_cv$splits[[1]]
the_cv$splits[[1]]
```

```{r}
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[1]] %>% training())
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[2]] %>% training())
lm(SalaryCY ~ Years + Title, data=the_cv$splits[[3]] %>% training())
```

# Feature Engineering

```{r}
library(recipes)
library(tune)

train$Title %>% table()

rec1 <- recipe(SalaryCY ~ ., data=train) %>% 
    step_rm(ID, SalaryPY, BonusPY, BonusCY) %>% 
    step_nzv(all_predictors()) %>% 
    step_knnimpute(all_predictors()) %>% 
    step_naomit(all_outcomes()) %>% 
    step_BoxCox(Floor) %>% 
    step_bs(Years, deg_free=tune()) %>% 
    step_normalize(all_numeric(), -SalaryCY) %>% 
    step_upsample(Title, over_ratio=0.5) %>% 
    step_other(all_nominal()) %>% 
    step_dummy(all_nominal(), one_hot=TRUE)

rec1

rec1 %>% parameters()

# won't work because of tuning parameter
# prep(rec1, training=train)
```

# Model Specification

```{r eval=FALSE}
lm(formula=y ~ x, data=data)
glm(formula=y ~ x, data=data)
glmnet(x=x_matrix, y=y_matrix)
xgb.train(data=xgb.DMatrix(data=x_matrix, label=y_matrix))
```

1. type of model
1. computation engine
1. fitting the model

```{r}
library(parsnip)

linear_reg()
logistic_reg()

decision_tree()
decision_tree(mode='regression')
decision_tree(mode='classification')

rand_forest()

boost_tree(mode='regression')
```

```{r}
linear_reg() %>% set_engine('lm')
linear_reg() %>% set_engine('glmnet')
linear_reg() %>% set_engine('stan')
linear_reg() %>% set_engine('keras')
linear_reg() %>% set_engine('spark')
```

```{r}
lm_spec <- linear_reg() %>% set_engine('lm')

mod1 <- lm_spec %>% fit(SalaryCY ~ Years + Title, data=train)
mod1
mod1 %>% class

net_spec <- linear_reg() %>% set_engine('glmnet')
mod2 <- net_spec %>% fit(SalaryCY ~ Years + Title, data=train)
mod2 %>% class
library(coefplot)
mod2$fit %>% coefpath()
```


```{r}
lm_spec %>% fit(SalaryCY ~ Years + Title, data=train)
net_spec %>% fit(SalaryCY ~ Years + Title, data=train)
```

```{r eval=FALSE}
# won't work because of tuning parameter
net_spec %>% 
    fit(
        SalaryCY ~ ., 
        data=rec1 %>% prep(training=train) %>% bake(newdata=train)
    )
```


```{r}
linear_reg(penalty=6537, mixture=1) %>% set_engine('glmnet')

boost_tree(trees=100)

net_spec <- linear_reg(penalty=tune(), mixture=1) %>% set_engine('glmnet')
net_spec %>% parameters()
```


